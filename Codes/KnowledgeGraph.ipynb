{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210adc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85a87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../Datasets/RamayanaEngSen.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a5b63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6011a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896912105d5a46feb5e4034b36678a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 23:27:13 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-04-26 23:27:14 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-04-26 23:27:14 INFO: Use device: cpu\n",
      "2022-04-26 23:27:14 INFO: Loading: tokenize\n",
      "2022-04-26 23:27:14 INFO: Loading: pos\n",
      "2022-04-26 23:27:14 INFO: Loading: lemma\n",
      "2022-04-26 23:27:14 INFO: Loading: depparse\n",
      "2022-04-26 23:27:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f80304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "    nl = nlp(sent)\n",
    "    for sen in nl.sentences:\n",
    "        for word in sen.words:\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "            if word.deprel != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "                if word.deprel == \"compound\":\n",
    "                    prefix = word.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        prefix = prv_tok_text + \" \"+ word.text\n",
    "                # check: token is a modifier or not\n",
    "                if word.deprel.endswith(\"mod\") == True:\n",
    "                    modifier = word.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        modifier = prv_tok_text + \" \"+ word.text\n",
    "\n",
    "      ## chunk 3\n",
    "                if word.deprel.find(\"subj\") == True:\n",
    "                    ent1 = modifier +\" \"+ prefix + \" \"+ word.text\n",
    "                    prefix = \"\"\n",
    "                    modifier = \"\"\n",
    "                    prv_tok_dep = \"\"\n",
    "                    prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "                if word.deprel.find(\"obj\") == 0:\n",
    "                    ent2 = modifier +\" \"+ prefix +\" \"+ word.text\n",
    "\n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "                prv_tok_dep = word.deprel\n",
    "                prv_tok_text = word.text\n",
    "  #############################################################\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b2bc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "for i in data[:50]:\n",
    "    entity_pairs.append(get_entities(nlp(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1a9b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "def get_relation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "    verb = ''\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == 'VERB' and word.deprel == 'root':\n",
    "                verb = word.text\n",
    "            if len(verb) == 0 and word.upos == 'VERB':\n",
    "                verb = word.text\n",
    "        return verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd1d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "for sent in data[:50]:\n",
    "    relations.append(get_relation(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71e1d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "source = [i[0] for i in entity_pairs]\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f90c7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01f0a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc53da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
